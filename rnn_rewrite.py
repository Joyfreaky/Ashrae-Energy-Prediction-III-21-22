# -*- coding: utf-8 -*-
"""RNN_rewrite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tpV30gu9xLNAFd50wK2omaUoGMz1-BzO
"""

# Imports
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot as plt
import datetime
import time
import requests
import io
import numpy as np

# Get data from drive
from google.colab import drive
drive.mount('/content/drive', force_remount = True)
df_train = pd.read_csv('/content/drive/My Drive/train.csv')
df_test = pd.read_csv('/content/drive/My Drive/test.csv')

# Keep useful columns
try:
  df_train = df_train.drop('timestamp', axis = 1)
  df_test = df_test.drop('timestamp', axis = 1)

  # EXPERIMENTAL - DROPPING METER
  df_train = df_train.drop('meter', axis = 1)
except:
  pass

df_train.isna().sum()

df_train

def clean_dataset(df):
    assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"
    df.dropna(inplace=True)
    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)
    return df[indices_to_keep].astype(np.float64)

df_train = clean_dataset(df_train)

# Split data
ratio = round(0.8 * df_train.shape[0])
train_values = df_train.iloc[:ratio, :]
test_values = df_train.iloc[ratio:, :]
print('training data size: ', train_values.shape, '\n    test data size: ', test_values.shape)

# Reduce memory usage
try:
  train_values = train_values.apply(float.half)
except:
  pass


train_values

from pandas import concat

# convert series to supervised learning by Jason Brownlee 
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	"""
	Frame a time series as a supervised learning dataset.
	Arguments:
		data: Sequence of observations as a list or NumPy array.
		n_in: Number of lag observations as input (X).
		n_out: Number of observations as output (y).
		dropnan: Boolean whether or not to drop rows with NaN values.
	Returns:
		Pandas DataFrame of series framed for supervised learning.
	"""
    
	n_vars = 1 if type(data) is list else data.shape[1]
	df = pd.DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg

# Scaling
scaler = MinMaxScaler(feature_range = (0, 1))
train_scaled = scaler.fit_transform(train_values)
test_scaled = scaler.fit_transform(test_values)

# Lags and features
lags = 2
features = train_scaled.shape[1] # Changed from test_scaled to train_scaled 01:01 19/04

# Frame for supervised learning
train_reframed = series_to_supervised(train_scaled, lags, 1)
test_reframed = series_to_supervised(test_scaled, lags, 1)

print(train_reframed.head())
print(test_reframed.head())

np.shape(train_scaled)

# Split inputs and outputs
values_train = train_reframed.values
values_test = test_reframed.values

# Split into x train, y train, etc
X_train, y_train = values_train[:, : -1], values_train[: ,1]
X_test, y_test = values_test[:, :-1], values_test[:, -1]
print(X_train.shape, X_test.shape)


# Reshape input to 3D (samples, timesteps, features)
X_train = X_train.reshape((X_train.shape[0], -1, 5))
X_test = X_test.reshape((X_test.shape[0], -1, 5))

print(X_train.shape)
print(X_test.shape)

np.shape(X_train)

# Define/Train LSTM
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

# Form network
model = Sequential()
model.add(LSTM(50, input_shape = (X_train.shape[1], X_train.shape[2])))
model.add(Dense(1))
model.compile(loss = 'mae', optimizer = 'adam')

# Fit network
history = model.fit(X_train, y_train, epochs = 1, batch_size = 72, validation_data = (X_test, y_test), verbose = 2, shuffle = False)

from math import sqrt
from numpy import concatenate
from sklearn.metrics import r2_score

yhat = model.predict(X_test)
r2_score(yhat, y_test)