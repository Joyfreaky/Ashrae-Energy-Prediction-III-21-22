{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tT3oRvEFUtYK"
      },
      "outputs": [],
      "source": [
        "# Energy prediction 3 (ASHRAE)\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.signal import savgol_filter as sg\n",
        "import holidays\n",
        "from pandas.api.types import is_categorical_dtype\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np  # linear algebra\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "black_day = 10\n",
        "\n",
        "debug = False\n",
        "num_rounds = 200\n",
        "\n",
        "folds = 3  # 3, 6, 12\n",
        "\n",
        "ucf_year = [2017, 2018]  # ucf data year used in train\n",
        "\n",
        "predmode = 'all'  \n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras import backend as k\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras.layers as layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import *\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
        "# Modified to support timestamp type, categorical type\n",
        "# Modified to add option to use float16 or not. feather format does not support float16.\n",
        "\n",
        "\n",
        "def reduce_mem_usage(df, use_float16=False):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "\n",
        "    for col in df.columns:\n",
        "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
        "            # skip datetime type or categorical type\n",
        "            continue\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(\n",
        "        100 * (start_mem - end_mem) / start_mem))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "zone_dict = {0: 4, 1: 0, 2: 7, 3: 4, 4: 7, 5: 0, 6: 4, 7: 4,\n",
        "             8: 4, 9: 5, 10: 7, 11: 4, 12: 0, 13: 5, 14: 4, 15: 4}\n",
        "\n",
        "\n",
        "def set_local(df):\n",
        "    for sid, zone in zone_dict.items():\n",
        "        sids = df.site_id == sid\n",
        "        df.loc[sids, 'timestamp'] = df[sids].timestamp - pd.offsets.Hour(zone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site_id</th>\n",
              "      <th>building_id</th>\n",
              "      <th>primary_use</th>\n",
              "      <th>square_feet</th>\n",
              "      <th>year_built</th>\n",
              "      <th>floor_count</th>\n",
              "      <th>meter</th>\n",
              "      <th>groupNum_train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Education</td>\n",
              "      <td>7432</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Education</td>\n",
              "      <td>2720</td>\n",
              "      <td>2004.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>Education</td>\n",
              "      <td>5376</td>\n",
              "      <td>1991.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Education</td>\n",
              "      <td>23685</td>\n",
              "      <td>2002.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>Education</td>\n",
              "      <td>116607</td>\n",
              "      <td>1975.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2375</th>\n",
              "      <td>15</td>\n",
              "      <td>1444</td>\n",
              "      <td>Entertainment/public assembly</td>\n",
              "      <td>19619</td>\n",
              "      <td>1914.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2376</th>\n",
              "      <td>15</td>\n",
              "      <td>1445</td>\n",
              "      <td>Education</td>\n",
              "      <td>4298</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2377</th>\n",
              "      <td>15</td>\n",
              "      <td>1446</td>\n",
              "      <td>Entertainment/public assembly</td>\n",
              "      <td>11265</td>\n",
              "      <td>1997.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2378</th>\n",
              "      <td>15</td>\n",
              "      <td>1447</td>\n",
              "      <td>Lodging/residential</td>\n",
              "      <td>29775</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2379</th>\n",
              "      <td>15</td>\n",
              "      <td>1448</td>\n",
              "      <td>Office</td>\n",
              "      <td>92271</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2380 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      site_id  building_id                    primary_use  square_feet  \\\n",
              "0           0            0                      Education         7432   \n",
              "1           0            1                      Education         2720   \n",
              "2           0            2                      Education         5376   \n",
              "3           0            3                      Education        23685   \n",
              "4           0            4                      Education       116607   \n",
              "...       ...          ...                            ...          ...   \n",
              "2375       15         1444  Entertainment/public assembly        19619   \n",
              "2376       15         1445                      Education         4298   \n",
              "2377       15         1446  Entertainment/public assembly        11265   \n",
              "2378       15         1447            Lodging/residential        29775   \n",
              "2379       15         1448                         Office        92271   \n",
              "\n",
              "      year_built  floor_count  meter  groupNum_train  \n",
              "0         2008.0          NaN      0               0  \n",
              "1         2004.0          NaN      0               0  \n",
              "2         1991.0          NaN      0               0  \n",
              "3         2002.0          NaN      0               0  \n",
              "4         1975.0          NaN      0               0  \n",
              "...          ...          ...    ...             ...  \n",
              "2375      1914.0          NaN      0             150  \n",
              "2376         NaN          NaN      0             150  \n",
              "2377      1997.0          NaN      0             150  \n",
              "2378      2001.0          NaN      0             150  \n",
              "2379      2001.0          NaN      0             150  \n",
              "\n",
              "[2380 rows x 8 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "root = Path('/home/joydipb/Documents/CMT307-Coursework-2-Group-19') # Change the path to the source file path, use Memory_Management.py to generate files in feather format \n",
        "train_df = pd.read_feather(root/'train.feather')\n",
        "weather_train_df = pd.read_feather(root/'weather_train.feather')\n",
        "building_meta_df = pd.read_feather(root/'building_metadata.feather')\n",
        "\n",
        "building_meta_df = building_meta_df.merge(\n",
        "    train_df[['building_id', 'meter']].drop_duplicates(), on='building_id')\n",
        "\n",
        "# Set group  (site-meter) for training models\n",
        "\n",
        "building_meta_df['groupNum_train'] = building_meta_df['site_id'].astype(\n",
        "    'int')*10 + building_meta_df['meter'].astype('int')\n",
        "\n",
        "building_meta_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "building_meta_df['floor_area'] = building_meta_df.square_feet / \\\n",
        "    building_meta_df.floor_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Site Specific Holiday\n",
        "\n",
        "\n",
        "en_holidays = holidays.England()\n",
        "ir_holidays = holidays.Ireland()\n",
        "ca_holidays = holidays.Canada()\n",
        "us_holidays = holidays.UnitedStates()\n",
        "\n",
        "\n",
        "def add_holiyday(df_weather):\n",
        "    en_idx = df_weather.query('site_id == 1 or site_id == 5').index\n",
        "    ir_idx = df_weather.query('site_id == 12').index\n",
        "    ca_idx = df_weather.query('site_id == 7 or site_id == 11').index\n",
        "    us_idx = df_weather.query(\n",
        "        'site_id == 0 or site_id == 2 or site_id == 3 or site_id == 4 or site_id == 6 or site_id == 8 or site_id == 9 or site_id == 10 or site_id == 13 or site_id == 14 or site_id == 15').index\n",
        "\n",
        "    df_weather['IsHoliday'] = 0\n",
        "    df_weather.loc[en_idx, 'IsHoliday'] = df_weather.loc[en_idx,\n",
        "                                                         'timestamp'].apply(lambda x: en_holidays.get(x, default=0))\n",
        "    df_weather.loc[ir_idx, 'IsHoliday'] = df_weather.loc[ir_idx,\n",
        "                                                         'timestamp'].apply(lambda x: ir_holidays.get(x, default=0))\n",
        "    df_weather.loc[ca_idx, 'IsHoliday'] = df_weather.loc[ca_idx,\n",
        "                                                         'timestamp'].apply(lambda x: ca_holidays.get(x, default=0))\n",
        "    df_weather.loc[us_idx, 'IsHoliday'] = df_weather.loc[us_idx,\n",
        "                                                         'timestamp'].apply(lambda x: us_holidays.get(x, default=0))\n",
        "\n",
        "    holiday_idx = df_weather['IsHoliday'] != 0\n",
        "    df_weather.loc[holiday_idx, 'IsHoliday'] = 1\n",
        "    df_weather['IsHoliday'] = df_weather['IsHoliday'].astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_local(weather_train_df)\n",
        "add_holiyday(weather_train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = train_df.query('not (building_id == 954 & meter_reading == 0)')\n",
        "train_df = train_df.query('not (building_id == 1221 & meter_reading == 0)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "after 19456584\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "261"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Removing buildings with meter == 0 before first initial reading.\n",
        "train_df = train_df.query(\n",
        "    'not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20 18\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 681 & meter == 0 & timestamp <= \"2016-04-27\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 761 & meter == 0 & timestamp <= \"2016-09-02\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 799 & meter == 0 & timestamp <= \"2016-09-02\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 802 & meter == 0 & timestamp <= \"2016-08-24\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1073 & meter == 0 & timestamp <= \"2016-10-26\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1094 & meter == 0 & timestamp <= \"2016-09-08\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 29 & meter == 0 & timestamp <= \"2016-08-10\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 40 & meter == 0 & timestamp <= \"2016-06-04\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 45 & meter == 0 & timestamp <= \"2016-07\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 106 & meter == 0 & timestamp <= \"2016-11\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 107 & meter == 0 & timestamp >= \"2016-11-10\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 112 & meter == 0 & timestamp < \"2016-10-31 15\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 144 & meter == 0 & timestamp > \"2016-05-14\" & timestamp < \"2016-10-31\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 147 & meter == 0 & timestamp > \"2016-06-05 19\" & timestamp < \"2016-07-18 15\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 171 & meter == 0 & timestamp <= \"2016-07-05\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 177 & meter == 0 & timestamp > \"2016-06-04\" & timestamp < \"2016-06-25\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 258 & meter == 0 & timestamp > \"2016-09-26\" & timestamp < \"2016-12-12\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 258 & meter == 0 & timestamp > \"2016-08-30\" & timestamp < \"2016-09-08\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 258 & meter == 0 & timestamp > \"2016-09-18\" & timestamp < \"2016-09-25\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 260 & meter == 0 & timestamp <= \"2016-05-11\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 269 & meter == 0 & timestamp > \"2016-06-04\" & timestamp < \"2016-06-25\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 304 & meter == 0 & timestamp >= \"2016-11-20\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 545 & meter == 0 & timestamp > \"2016-01-17\" & timestamp < \"2016-02-10\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 604 & meter == 0 & timestamp < \"2016-11-21\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 693 & meter == 0 & timestamp > \"2016-09-07\" & timestamp < \"2016-11-23\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 693 & meter == 0 & timestamp > \"2016-07-12\" & timestamp < \"2016-05-29\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 723 & meter == 0 & timestamp > \"2016-10-06\" & timestamp < \"2016-11-22\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 733 & meter == 0 & timestamp > \"2016-05-29\" & timestamp < \"2016-06-22\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 733 & meter == 0 & timestamp > \"2016-05-19\" & timestamp < \"2016-05-20\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 803 & meter == 0 & timestamp > \"2016-9-25\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 815 & meter == 0 & timestamp > \"2016-05-17\" & timestamp < \"2016-11-17\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 848 & meter == 0 & timestamp > \"2016-01-15\" & timestamp < \"2016-03-20\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 857 & meter == 0 & timestamp > \"2016-04-13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 909 & meter == 0 & timestamp < \"2016-02-02\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 909 & meter == 0 & timestamp < \"2016-06-23\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1008 & meter == 0 & timestamp > \"2016-10-30\" & timestamp < \"2016-11-21\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1113 & meter == 0 & timestamp < \"2016-07-27\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1153 & meter == 0 & timestamp < \"2016-01-20\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1169 & meter == 0 & timestamp < \"2016-08-03\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1170 & meter == 0 & timestamp > \"2016-06-30\" & timestamp < \"2016-07-05\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1221 & meter == 0 & timestamp < \"2016-11-04\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1225 & meter == 0 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1234 & meter == 0 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1233 & building_id <= 1234 & meter == 0 & timestamp > \"2016-01-13 22\" & timestamp < \"2016-03-08 12\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1241 & meter == 0 & timestamp > \"2016-07-14\" & timestamp < \"2016-11-19\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1250 & meter == 0 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1255 & meter == 0 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1264 & meter == 0 & timestamp > \"2016-08-23\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1265 & meter == 0 & timestamp > \"2016-05-06\" & timestamp < \"2016-05-26\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1272 & meter == 0 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1275 & building_id <= 1280 & meter == 0 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1283 & meter == 0 & timestamp > \"2016-07-08\" & timestamp < \"2016-08-03\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1291 & building_id <= 1302 & meter == 0 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1303 & meter == 0 & timestamp > \"2016-07-25 22\" & timestamp < \"2016-07-27 16\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1303 & meter == 0 & timestamp > \"2016-01-26\" & timestamp < \"2016-06-02 12\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1319 & meter == 0 & timestamp > \"2016-05-17 16\" & timestamp < \"2016-06-07 12\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1319 & meter == 0 & timestamp > \"2016-08-18 14\" & timestamp < \"2016-09-02 14\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1322 & meter == 0 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "\n",
        "# 2nd cleaning\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 0 & timestamp > \"2016-10-14 22\" & timestamp < \"2016-10-17 08\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 0 & timestamp > \"2016-07-01 14\" & timestamp < \"2016-07-05 06\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 1 & timestamp > \"2016-10-14 22\" & timestamp < \"2016-10-17 08\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 1 & timestamp > \"2016-07-01 14\" & timestamp < \"2016-07-05 06\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 2 & timestamp > \"2016-10-14 22\" & timestamp < \"2016-10-17 08\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 2 & timestamp > \"2016-07-01 14\" & timestamp < \"2016-07-05 06\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1272 & meter == 1 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1291 & building_id <= 1297 & meter == 1 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1300 & meter == 1 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1302 & meter == 1 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1291 & building_id <= 1299 & meter == 2 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1221 & meter == 0 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1225 & building_id <= 1226 & meter == 0 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1233 & building_id <= 1234 & meter == 0 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1241 & meter == 0 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1223 & meter == 1 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1226 & meter == 1 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1233 & building_id <= 1234 & meter == 1 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1225 & building_id <= 1226 & meter == 2 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1305 & meter == 2 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1307 & meter == 2 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1223 & meter == 3 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1231 & meter == 3 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1233 & building_id <= 1234 & meter == 3 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1272 & meter == 3 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1275 & building_id <= 1297 & meter == 3 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1300 & meter == 3 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1302 & meter == 3 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1293 & meter == 3 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-25 12\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1302 & meter == 3 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-25 12\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1223 & meter == 0 & timestamp > \"2016-9-28 07\" & timestamp < \"2016-10-11 18\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1225 & meter == 1 & timestamp > \"2016-8-22 23\" & timestamp < \"2016-10-11 14\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1230 & meter == 1 & timestamp > \"2016-8-22 08\" & timestamp < \"2016-10-05 18\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 904 & meter == 0 & timestamp < \"2016-02-17 08\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 986 & meter == 0 & timestamp < \"2016-02-17 08\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 954 & meter == 0 & timestamp < \"2016-08-08 11\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 954 & meter == 0 & timestamp < \"2016-06-23 08\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 745 & building_id <= 770 & meter == 1 & timestamp > \"2016-10-05 01\" & timestamp < \"2016-10-10 09\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 774 & building_id <= 787 & meter == 1 & timestamp > \"2016-10-05 01\" & timestamp < \"2016-10-10 09\")')\n",
        "\n",
        "# 3rd cleaning hourly spikes\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 0 & timestamp > \"2016-05-11 09\" & timestamp < \"2016-05-12 01\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 1 & timestamp > \"2016-05-11 09\" & timestamp < \"2016-05-12 01\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 2 & timestamp > \"2016-05-11 09\" & timestamp < \"2016-05-12 01\")')\n",
        "\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 0 & timestamp == \"2016-02-26 01\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 1 & timestamp == \"2016-02-26 01\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 2 & timestamp == \"2016-02-26 01\")')\n",
        "\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 0 & timestamp > \"2016-03-29 10\" & timestamp < \"2016-03-30 12\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 1 & timestamp > \"2016-03-29 10\" & timestamp < \"2016-03-30 12\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 2 & timestamp > \"2016-03-29 10\" & timestamp < \"2016-03-30 12\")')\n",
        "\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 0 & timestamp > \"2016-01-19 23\" & timestamp < \"2016-01-28 15\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 1 & timestamp > \"2016-01-19 23\" & timestamp < \"2016-01-28 15\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 874 & building_id <= 997 & meter == 2 & timestamp > \"2016-01-19 23\" & timestamp < \"2016-01-28 15\")')\n",
        "\n",
        "train_df = train_df.query(\n",
        "    'not (building_id != 1227 & building_id != 1281 & building_id != 1314 & building_id >=1223 & building_id < 1335 & meter==0 & meter_reading==0)')\n",
        "\n",
        "# 4th cleaning\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1223 & building_id <= 1324 & meter==1 & timestamp > \"2016-07-16 04\" & timestamp < \"2016-07-19 11\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 107 & meter == 0 & timestamp <= \"2016-07-06\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 180 & timestamp >= \"2016-02-17 12\")')\n",
        "train_df = train_df.query('not (building_id == 182 & meter == 0)')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 191 & meter == 0 & timestamp >= \"2016-12-22 09\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 192 & meter == 1 & timestamp >= \"2016-05-09 18\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 192 & meter == 3 & timestamp >= \"2016-03-29 05\" & timestamp <= \"2016-04-04 08\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 207 & meter == 1 & timestamp > \"2016-07-02 20\" & timestamp < \"2016-08-25 12\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 258 & timestamp > \"2016-09-18\" & timestamp < \"2016-12-12 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 258 & timestamp > \"2016-08-29 08\" & timestamp < \"2016-09-08 14\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 257 & meter == 1 & timestamp < \"2016-03-25 16\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 260 & meter == 1 & timestamp > \"2016-05-10 17\" & timestamp < \"2016-08-17 11\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 260 & meter == 1 & timestamp > \"2016-08-28 01\" & timestamp < \"2016-10-31 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 220 & meter == 1 & timestamp > \"2016-09-23 01\" & timestamp < \"2016-09-23 12\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 281 & meter == 1 & timestamp > \"2016-10-25 08\" & timestamp < \"2016-11-04 15\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 273 & meter == 1 & timestamp > \"2016-04-03 04\" & timestamp < \"2016-04-29 15\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 28 & meter == 0 & timestamp < \"2016-10-14 20\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 71 & meter == 0 & timestamp < \"2016-08-18 20\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 76 & meter == 0 & timestamp > \"2016-06-04 09\" & timestamp < \"2016-06-04 14\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 101 & meter == 0 & timestamp > \"2016-10-12 13\" & timestamp < \"2016-10-12 18\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 7 & meter == 1 & timestamp > \"2016-11-03 09\" & timestamp < \"2016-11-28 14\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 9 & meter == 1 & timestamp > \"2016-12-06 08\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 43 & meter == 1 & timestamp > \"2016-04-03 08\" & timestamp < \"2016-06-06 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 60 & meter == 1 & timestamp > \"2016-05-01 17\" & timestamp < \"2016-05-01 21\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 75 & meter == 1 & timestamp > \"2016-08-05 13\" & timestamp < \"2016-08-26 12\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 95 & meter == 1 & timestamp > \"2016-08-08 10\" & timestamp < \"2016-08-26 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 97 & meter == 1 & timestamp > \"2016-08-08 14\" & timestamp < \"2016-08-25 14\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1232 & meter == 1 & timestamp > \"2016-06-23 16\" & timestamp < \"2016-08-31 20\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1236 & meter == 1 & meter_reading >= 3000)')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1239 & meter == 1 & timestamp > \"2016-03-11 16\" & timestamp < \"2016-03-27 17\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1264 & meter == 1 & timestamp > \"2016-08-22 17\" & timestamp < \"2016-09-22 20\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1264 & meter == 1 & timestamp > \"2016-09-28 07\" & timestamp < \"2016-10-20 13\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1269 & meter == 1 & meter_reading >= 2000)')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1272 & meter == 1 & timestamp > \"2016-08-11 12\" & timestamp < \"2016-08-30 19\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1273 & meter == 1 & timestamp > \"2016-05-31 14\" & timestamp < \"2016-06-17\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1276 & meter == 1 & timestamp < \"2016-02-03 23\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1280 & meter == 1 & timestamp > \"2016-05-18\" & timestamp < \"2016-05-26 09\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1280 & meter == 1 & timestamp > \"2016-02-28 23\" & timestamp < \"2016-05-02 05\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1280 & meter == 1 & timestamp > \"2016-06-12 01\" & timestamp < \"2016-7-07 06\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1288 & meter == 1 & timestamp > \"2016-07-07 15\" & timestamp < \"2016-08-12 17\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1311 & meter == 1 & timestamp > \"2016-04-25 18\" & timestamp < \"2016-05-13 14\")')\n",
        "train_df = train_df.query('not (building_id == 1099 & meter == 2)')\n",
        "\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1329 & meter == 0 & timestamp > \"2016-04-28 00\" & timestamp < \"2016-04-28 07\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1331 & meter == 0 & timestamp > \"2016-04-28 00\" & timestamp < \"2016-04-28 07\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1427 & meter == 0 & timestamp > \"2016-04-11 10\" & timestamp < \"2016-04-11 14\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1426 & meter == 2 & timestamp > \"2016-05-03 09\" & timestamp < \"2016-05-03 14\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1345 & meter == 0 & timestamp < \"2016-03-01\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1346 & timestamp < \"2016-03-01\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1359 & meter == 0 & timestamp > \"2016-04-25 17\" & timestamp < \"2016-07-22 14\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1365 & meter == 0 & timestamp > \"2016-08-19 00\" & timestamp < \"2016-08-19 07\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1365 & meter == 0 & timestamp > \"2016-06-18 22\" & timestamp < \"2016-06-19 06\")')\n",
        "\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 18 & meter == 0 & timestamp > \"2016-06-04 09\" & timestamp < \"2016-06-04 16\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 18 & meter == 0 & timestamp > \"2016-11-05 05\" & timestamp < \"2016-11-05 15\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 101 & meter == 0 & meter_reading > 800)')\n",
        "\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1384 & meter == 0 & meter_reading == 0 )')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1289 & building_id <= 1301 & meter == 2 & meter_reading == 0)')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1243 & meter == 2 & meter_reading == 0)')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1263 & meter == 2 & meter_reading == 0)')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1284 & meter == 2 & meter_reading == 0)')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1286 & meter == 2 & meter_reading == 0)')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1263 & meter == 0 & timestamp > \"2016-11-10 11\" & timestamp < \"2016-11-10 15\")')\n",
        "\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1238 & meter == 2 & meter_reading == 0)')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1329 & meter == 2 & timestamp > \"2016-11-21 12\" & timestamp < \"2016-11-29 12\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1249 & meter == 2 & meter_reading == 0)')\n",
        "\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1250 & meter == 2 & meter_reading == 0)')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1256 & meter == 2 & timestamp > \"2016-03-05 18\" & timestamp < \"2016-03-05 22\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1256 & meter == 2 & timestamp > \"2016-03-27 00\" & timestamp < \"2016-03-27 23\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1256 & meter == 2 & timestamp > \"2016-04-11 09\" & timestamp < \"2016-04-13 03\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1256 & meter == 2 & timestamp > \"2016-04-29 00\" & timestamp < \"2016-04-30 15\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id == 1303 & meter == 2 & timestamp < \"2016-06-06 19\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1223 & building_id <= 1324 & meter == 1 & timestamp > \"2016-08-11 17\" & timestamp < \"2016-08-12 17\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1223 & building_id <= 1324 & building_id != 1296 & building_id != 129 & building_id != 1298 & building_id != 1299 & meter == 2 & timestamp > \"2016-08-11 17\" & timestamp < \"2016-08-12 17\")')\n",
        "train_df = train_df.query(\n",
        "    'not (building_id >= 1223 & building_id <= 1324 & meter == 3 & timestamp > \"2016-08-11 17\" & timestamp < \"2016-08-12 17\")')\n",
        "\n",
        "train_df = train_df.reset_index()\n",
        "\n",
        "print('after', len(train_df))\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "105 105\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>building_id</th>\n",
              "      <th>meter</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>meter_reading</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7054137</th>\n",
              "      <td>7579069</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>277.802002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054138</th>\n",
              "      <td>7579070</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>125.863998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054139</th>\n",
              "      <td>7579071</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>32.762901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054140</th>\n",
              "      <td>7579072</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>336.843994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054141</th>\n",
              "      <td>7579073</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>1625.859985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054142</th>\n",
              "      <td>7579074</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>45.390301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054143</th>\n",
              "      <td>7579075</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>83.545403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054144</th>\n",
              "      <td>7579076</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>511.920013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054146</th>\n",
              "      <td>7579078</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>348.787994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054147</th>\n",
              "      <td>7579079</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>120.813004</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           index  building_id  meter           timestamp  meter_reading\n",
              "7054137  7579069            0      0 2016-05-20 19:00:00     277.802002\n",
              "7054138  7579070            1      0 2016-05-20 19:00:00     125.863998\n",
              "7054139  7579071            2      0 2016-05-20 19:00:00      32.762901\n",
              "7054140  7579072            3      0 2016-05-20 19:00:00     336.843994\n",
              "7054141  7579073            4      0 2016-05-20 19:00:00    1625.859985\n",
              "7054142  7579074            5      0 2016-05-20 19:00:00      45.390301\n",
              "7054143  7579075            6      0 2016-05-20 19:00:00      83.545403\n",
              "7054144  7579076            7      0 2016-05-20 19:00:00     511.920013\n",
              "7054146  7579078            8      0 2016-05-20 19:00:00     348.787994\n",
              "7054147  7579079            9      0 2016-05-20 19:00:00     120.813004"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Site-0 CorrectionÂ¶\n",
        "# https://www.kaggle.com/c/ashrae-energy-prediction/discussion/119261#latest-684102\n",
        "site_0_bids = building_meta_df[building_meta_df.site_id ==\n",
        "                               0].building_id.unique()\n",
        "print(len(site_0_bids), len(\n",
        "    train_df[train_df.building_id.isin(site_0_bids)].building_id.unique()))\n",
        "train_df[train_df.building_id.isin(\n",
        "    site_0_bids) & (train_df.meter == 0)].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.loc[(train_df.building_id.isin(site_0_bids)) & (train_df.meter == 0), 'meter_reading'] = train_df[(\n",
        "    train_df.building_id.isin(site_0_bids)) & (train_df.meter == 0)]['meter_reading'] * 0.2931"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>building_id</th>\n",
              "      <th>meter</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>meter_reading</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7054137</th>\n",
              "      <td>7579069</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>81.423767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054138</th>\n",
              "      <td>7579070</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>36.890739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054139</th>\n",
              "      <td>7579071</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>9.602806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054140</th>\n",
              "      <td>7579072</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>98.728973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054141</th>\n",
              "      <td>7579073</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>476.539551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054142</th>\n",
              "      <td>7579074</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>13.303897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054143</th>\n",
              "      <td>7579075</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>24.487158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054144</th>\n",
              "      <td>7579076</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>150.043762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054146</th>\n",
              "      <td>7579078</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>102.229759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7054147</th>\n",
              "      <td>7579079</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-05-20 19:00:00</td>\n",
              "      <td>35.410290</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           index  building_id  meter           timestamp  meter_reading\n",
              "7054137  7579069            0      0 2016-05-20 19:00:00      81.423767\n",
              "7054138  7579070            1      0 2016-05-20 19:00:00      36.890739\n",
              "7054139  7579071            2      0 2016-05-20 19:00:00       9.602806\n",
              "7054140  7579072            3      0 2016-05-20 19:00:00      98.728973\n",
              "7054141  7579073            4      0 2016-05-20 19:00:00     476.539551\n",
              "7054142  7579074            5      0 2016-05-20 19:00:00      13.303897\n",
              "7054143  7579075            6      0 2016-05-20 19:00:00      24.487158\n",
              "7054144  7579076            7      0 2016-05-20 19:00:00     150.043762\n",
              "7054146  7579078            8      0 2016-05-20 19:00:00     102.229759\n",
              "7054147  7579079            9      0 2016-05-20 19:00:00      35.410290"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[(train_df.building_id.isin(site_0_bids))\n",
        "         & (train_df.meter == 0)].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df['date'] = train_df['timestamp'].dt.date\n",
        "train_df['meter_reading_log1p'] = np.log1p(train_df['meter_reading'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(df):\n",
        "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "    df[\"day\"] = df[\"timestamp\"].dt.day\n",
        "    df[\"weekend\"] = df[\"timestamp\"].dt.weekday\n",
        "    df[\"month\"] = df[\"timestamp\"].dt.month\n",
        "    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocess(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "weather_train_df = weather_train_df.groupby('site_id').apply(\n",
        "    lambda group: group.interpolate(method='ffill', limit_direction='forward'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adding some lag feature\n",
        "\n",
        "def add_lag_feature(weather_df, window=3):\n",
        "    group_df = weather_df.groupby('site_id')\n",
        "    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature',\n",
        "            'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n",
        "    rolled = group_df[cols].rolling(window=window, min_periods=0)\n",
        "    lag_mean = rolled.mean().reset_index().astype(np.float16)\n",
        "    lag_max = rolled.max().reset_index().astype(np.float16)\n",
        "    lag_min = rolled.min().reset_index().astype(np.float16)\n",
        "    lag_std = rolled.std().reset_index().astype(np.float16)\n",
        "    for col in cols:\n",
        "        weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n",
        "        weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n",
        "        weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n",
        "        weather_df[f'{col}_std_lag{window}'] = lag_std[col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "add_lag_feature(weather_train_df, window=3)\n",
        "add_lag_feature(weather_train_df, window=72)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "year_map = building_meta_df.year_built.value_counts()\n",
        "building_meta_df['year_cnt'] = building_meta_df.year_built.map(year_map)\n",
        "\n",
        "bid_map = train_df.building_id.value_counts()\n",
        "train_df['bid_cnt'] = train_df.building_id.map(bid_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "primary_use_dict:  {'Education': 0, 'Lodging/residential': 1, 'Office': 2, 'Entertainment/public assembly': 3, 'Other': 4, 'Retail': 5, 'Parking': 6, 'Public services': 7, 'Warehouse/storage': 8, 'Food sales and service': 9, 'Religious worship': 10, 'Healthcare': 11, 'Utility': 12, 'Technology/science': 13, 'Manufacturing/industrial': 14, 'Services': 15}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "primary_use_list = building_meta_df['primary_use'].unique()\n",
        "primary_use_dict = {key: value for value, key in enumerate(primary_use_list)}\n",
        "print('primary_use_dict: ', primary_use_dict)\n",
        "building_meta_df['primary_use'] = building_meta_df['primary_use'].map(\n",
        "    primary_use_dict)\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 1540.09 MB\n",
            "Memory usage after optimization is: 593.78 MB\n",
            "Decreased by 61.4%\n",
            "Memory usage of dataframe is 0.11 MB\n",
            "Memory usage after optimization is: 0.07 MB\n",
            "Decreased by 40.6%\n",
            "Memory usage of dataframe is 21.06 MB\n",
            "Memory usage after optimization is: 19.33 MB\n",
            "Decreased by 8.2%\n"
          ]
        }
      ],
      "source": [
        "train_df = reduce_mem_usage(train_df, use_float16=True)\n",
        "building_meta_df = reduce_mem_usage(building_meta_df, use_float16=True)\n",
        "weather_train_df = reduce_mem_usage(weather_train_df, use_float16=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SG Filter for Weather\n",
        "\n",
        "def add_sg(df):\n",
        "    w = 11\n",
        "    p = 2\n",
        "    for si in df.site_id.unique():\n",
        "        index = df.site_id == si\n",
        "        df.loc[index, 'air_smooth'] = sg(df[index].air_temperature, w, p)\n",
        "        df.loc[index, 'dew_smooth'] = sg(df[index].dew_temperature, w, p)\n",
        "\n",
        "        df.loc[index, 'air_diff'] = sg(df[index].air_temperature, w, p, 1)\n",
        "        df.loc[index, 'dew_diff'] = sg(df[index].dew_temperature, w, p, 1)\n",
        "\n",
        "        df.loc[index, 'air_diff2'] = sg(df[index].air_temperature, w, p, 2)\n",
        "        df.loc[index, 'dew_diff2'] = sg(df[index].dew_temperature, w, p, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "add_sg(weather_train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Selection\n",
        "\n",
        "category_cols = ['building_id', 'site_id', 'primary_use',\n",
        "                 'IsHoliday', 'groupNum_train']  # , 'meter'\n",
        "feature_cols = ['square_feet_np_log1p', 'year_built'] + [\n",
        "    'hour', 'weekend',\n",
        "    #    'day', # 'month' ,\n",
        "    #    'dayofweek',\n",
        "    #    'building_median'\n",
        "    #    'square_feet'\n",
        "] + [\n",
        "    'air_temperature', 'cloud_coverage',\n",
        "    'dew_temperature', 'precip_depth_1_hr',\n",
        "    'sea_level_pressure',\n",
        "    #'wind_direction', 'wind_speed',\n",
        "    'air_temperature_mean_lag72',\n",
        "    'air_temperature_max_lag72', 'air_temperature_min_lag72',\n",
        "    'air_temperature_std_lag72', 'cloud_coverage_mean_lag72',\n",
        "    'dew_temperature_mean_lag72', 'precip_depth_1_hr_mean_lag72',\n",
        "    'sea_level_pressure_mean_lag72',\n",
        "    # 'wind_direction_mean_lag72',\n",
        "    'wind_speed_mean_lag72',\n",
        "    'air_temperature_mean_lag3',\n",
        "    'air_temperature_max_lag3',\n",
        "    'air_temperature_min_lag3', 'cloud_coverage_mean_lag3',\n",
        "    'dew_temperature_mean_lag3',\n",
        "    'precip_depth_1_hr_mean_lag3',\n",
        "    'sea_level_pressure_mean_lag3',\n",
        "    #    'wind_direction_mean_lag3', 'wind_speed_mean_lag3',\n",
        "    #    'floor_area',\n",
        "    'year_cnt', 'bid_cnt',\n",
        "    'dew_smooth', 'air_smooth',\n",
        "    'dew_diff', 'air_diff',\n",
        "    'dew_diff2', 'air_diff2'\n",
        " ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = train_df.merge(building_meta_df, on=[\n",
        "                          'building_id', 'meter'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 1076.22 MB\n",
            "Memory usage after optimization is: 1076.22 MB\n",
            "Decreased by 0.0%\n",
            "Memory usage of dataframe is 25.73 MB\n",
            "Memory usage after optimization is: 20.93 MB\n",
            "Decreased by 18.7%\n"
          ]
        }
      ],
      "source": [
        "train_df = reduce_mem_usage(train_df, use_float16=True)\n",
        "weather_train_df = reduce_mem_usage(weather_train_df, use_float16=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = train_df.merge(weather_train_df, on=[\n",
        "                          'site_id', 'timestamp'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df['square_feet_np_log1p'] = np.log1p(train_df['square_feet'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 3822.39 MB\n",
            "Memory usage after optimization is: 3711.06 MB\n",
            "Decreased by 2.9%\n"
          ]
        }
      ],
      "source": [
        "train_df = reduce_mem_usage(train_df, use_float16=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del weather_train_df\n",
        "gc.collect() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "iyt_1Xg7cg1X",
        "outputId": "daf36014-f11c-4cda-f7ca-d6040a35353c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n--------------------------------------------------------------------------------\\nIMPLEMENTATION\\n'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "--------------------------------------------------------------------------------\n",
        "IMPLEMENTATION\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KVLutpqjcuoo"
      },
      "outputs": [],
      "source": [
        "# Get X_train and y_train\n",
        "def create_X_y(train_df, groupNum_train):\n",
        "\n",
        "    target_train_df = train_df[train_df['groupNum_train']\n",
        "                               == groupNum_train].copy()\n",
        "    \n",
        "    X_train = target_train_df[feature_cols + category_cols]\n",
        "    y_train = target_train_df['meter_reading_log1p'].values\n",
        "\n",
        "    del target_train_df\n",
        "    return X_train, y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kBX5P0Phedaw"
      },
      "outputs": [],
      "source": [
        "# # Define RMSLE specifically for neural network\n",
        "def NN_RMSLE(y_act, y_pred):\n",
        "  return k.sqrt(k.mean(k.square(y_pred - y_act)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vV-tpC9pfegY"
      },
      "outputs": [],
      "source": [
        "# Split into train and validation sets\n",
        "# train_xx, val_xx, train_yy, val_yy = train_test_split(train_x, train_y, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method to train LSTM model\n",
        "def RNN_LSTM(train, val ):\n",
        "       X_train, y_train = train\n",
        "       X_valid, y_valid = val\n",
        "       metric= 'val_loss'\n",
        "       model = Sequential()\n",
        "       early_stop = EarlyStopping(monitor = metric, mode = 'min', patience = 3)\n",
        "       model_checkpoint = ModelCheckpoint(\"model_\" + str(groupNum_train) + \"[]\",\n",
        "                                          save_best_only=True, verbose=1, monitor=metric, mode='min')\n",
        "\n",
        "       # Add layers, etc\n",
        "       model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "                      return_sequences=True))\n",
        "       model.add(Dropout(0.2))\n",
        "       model.add(LSTM(64, return_sequences=True))\n",
        "       model.add(Dropout(0.2))\n",
        "       model.add(LSTM(32))\n",
        "       model.add(Dropout(0.2))\n",
        "       model.add(Dense(1, activation='linear'))\n",
        "       model.compile(loss='mse', optimizer='adam', metrics=[NN_RMSLE])\n",
        "       model.summary()\n",
        "\n",
        "       # Fit model\n",
        "       model.fit(X_train, y_train, epochs=100, batch_size=128,\n",
        "                 validation_data=(X_valid, y_valid),\n",
        "                 callbacks=[early_stop, model_checkpoint], verbose=1)\n",
        "\n",
        "       y_pred_valid = model.predict(X_valid)\n",
        "\n",
        "       print('---------- Evaluation on Training Data ----------')\n",
        "       print(\"MSE: \", mean_squared_error(y_valid, y_pred_valid))\n",
        "       print(\"\")\n",
        "       \n",
        "       return model, y_pred_valid\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "seed = 666\n",
        "shuffle = False\n",
        "kf = StratifiedKFold(n_splits=folds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "groupNum_train 0 (553329, 38)\n",
            "cat_features [33, 34, 35, 36, 37]\n",
            "[ 5  6  7  8  9 10 11]\n",
            "train 368886 valid 184443\n",
            "Epoch 1/10\n",
            "36/37 [============================>.] - ETA: 0s - loss: 5.4906\n",
            "Epoch 1: val_loss improved from inf to 4.21127, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 2s 46ms/step - loss: 5.4602 - val_loss: 4.2113\n",
            "Epoch 2/10\n",
            " 1/37 [..............................] - ETA: 0s - loss: 4.2483"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-25 05:10:55.242495: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37/37 [==============================] - ETA: 0s - loss: 4.2143\n",
            "Epoch 2: val_loss improved from 4.21127 to 4.17563, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 2s 47ms/step - loss: 4.2143 - val_loss: 4.1756\n",
            "Epoch 3/10\n",
            "37/37 [==============================] - ETA: 0s - loss: 4.1788\n",
            "Epoch 3: val_loss improved from 4.17563 to 4.14014, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 2s 49ms/step - loss: 4.1788 - val_loss: 4.1401\n",
            "Epoch 4/10\n",
            "36/37 [============================>.] - ETA: 0s - loss: 4.1438\n",
            "Epoch 4: val_loss improved from 4.14014 to 4.10475, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 2s 42ms/step - loss: 4.1433 - val_loss: 4.1048\n",
            "Epoch 5/10\n",
            "37/37 [==============================] - ETA: 0s - loss: 4.1080\n",
            "Epoch 5: val_loss improved from 4.10475 to 4.06944, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 35ms/step - loss: 4.1080 - val_loss: 4.0694\n",
            "Epoch 6/10\n",
            "37/37 [==============================] - ETA: 0s - loss: 4.0727\n",
            "Epoch 6: val_loss improved from 4.06944 to 4.03420, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 40ms/step - loss: 4.0727 - val_loss: 4.0342\n",
            "Epoch 7/10\n",
            "36/37 [============================>.] - ETA: 0s - loss: 4.0375\n",
            "Epoch 7: val_loss improved from 4.03420 to 3.99902, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 2s 44ms/step - loss: 4.0375 - val_loss: 3.9990\n",
            "Epoch 8/10\n",
            "36/37 [============================>.] - ETA: 0s - loss: 4.0028\n",
            "Epoch 8: val_loss improved from 3.99902 to 3.96389, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 2s 52ms/step - loss: 4.0024 - val_loss: 3.9639\n",
            "Epoch 9/10\n",
            "37/37 [==============================] - ETA: 0s - loss: 3.9673\n",
            "Epoch 9: val_loss improved from 3.96389 to 3.92882, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 2s 44ms/step - loss: 3.9673 - val_loss: 3.9288\n",
            "Epoch 10/10\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 3.9332\n",
            "Epoch 10: val_loss improved from 3.92882 to 3.89381, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 35ms/step - loss: 3.9323 - val_loss: 3.8938\n",
            "---------- Evaluation on Training Data ----------\n",
            "MSE:  15.162771\n",
            "\n",
            "[ 8  9 10 11 12]\n",
            "train 368886 valid 184443\n",
            "Epoch 1/10\n",
            "37/37 [==============================] - ETA: 0s - loss: 9.9908 \n",
            "Epoch 1: val_loss improved from inf to 4.26316, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 2s 42ms/step - loss: 9.9908 - val_loss: 4.2632\n",
            "Epoch 2/10\n",
            "36/37 [============================>.] - ETA: 0s - loss: 4.1976\n",
            "Epoch 2: val_loss improved from 4.26316 to 4.22776, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 34ms/step - loss: 4.1967 - val_loss: 4.2278\n",
            "Epoch 3/10\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 4.1624\n",
            "Epoch 3: val_loss improved from 4.22776 to 4.19236, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 38ms/step - loss: 4.1615 - val_loss: 4.1924\n",
            "Epoch 4/10\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 4.1278\n",
            "Epoch 4: val_loss improved from 4.19236 to 4.15699, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 32ms/step - loss: 4.1263 - val_loss: 4.1570\n",
            "Epoch 5/10\n",
            "37/37 [==============================] - ETA: 0s - loss: 4.0912\n",
            "Epoch 5: val_loss improved from 4.15699 to 4.12165, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 36ms/step - loss: 4.0912 - val_loss: 4.1217\n",
            "Epoch 6/10\n",
            "37/37 [==============================] - ETA: 0s - loss: 4.0560\n",
            "Epoch 6: val_loss improved from 4.12165 to 4.08637, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 33ms/step - loss: 4.0560 - val_loss: 4.0864\n",
            "Epoch 7/10\n",
            "37/37 [==============================] - ETA: 0s - loss: 4.0210\n",
            "Epoch 7: val_loss improved from 4.08637 to 4.05113, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 35ms/step - loss: 4.0210 - val_loss: 4.0511\n",
            "Epoch 8/10\n",
            "37/37 [==============================] - ETA: 0s - loss: 3.9860\n",
            "Epoch 8: val_loss improved from 4.05113 to 4.01592, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 32ms/step - loss: 3.9860 - val_loss: 4.0159\n",
            "Epoch 9/10\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 3.9520\n",
            "Epoch 9: val_loss improved from 4.01592 to 3.98077, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 36ms/step - loss: 3.9510 - val_loss: 3.9808\n",
            "Epoch 10/10\n",
            "36/37 [============================>.] - ETA: 0s - loss: 3.9166\n",
            "Epoch 10: val_loss improved from 3.98077 to 3.94567, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 36ms/step - loss: 3.9161 - val_loss: 3.9457\n",
            "---------- Evaluation on Training Data ----------\n",
            "MSE:  15.569861\n",
            "\n",
            "[10 11 12]\n",
            "train 368886 valid 184443\n",
            "Epoch 1/10\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 65.0962\n",
            "Epoch 1: val_loss improved from inf to 53.03352, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 2s 39ms/step - loss: 64.0100 - val_loss: 53.0335\n",
            "Epoch 2/10\n",
            "37/37 [==============================] - ETA: 0s - loss: 44.7394\n",
            "Epoch 2: val_loss improved from 53.03352 to 20.43233, saving model to model_0[]\n",
            "INFO:tensorflow:Assets written to: model_0[]/assets\n",
            "37/37 [==============================] - 1s 31ms/step - loss: 44.7394 - val_loss: 20.4323\n",
            "Epoch 3/10\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 43.1982\n",
            "Epoch 3: val_loss did not improve from 20.43233\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 42.8460 - val_loss: 37.5682\n",
            "Epoch 4/10\n",
            "37/37 [==============================] - ETA: 0s - loss: 42.9800\n",
            "Epoch 4: val_loss did not improve from 20.43233\n",
            "37/37 [==============================] - 1s 25ms/step - loss: 42.9800 - val_loss: 59.5921\n",
            "Epoch 5/10\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 43.3967\n",
            "Epoch 5: val_loss did not improve from 20.43233\n",
            "37/37 [==============================] - 1s 24ms/step - loss: 42.8574 - val_loss: 47.4728\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/joydipb/Documents/CMT307-Coursework-2-Group-19/LSTM_RNN_Final.ipynb Cell 37'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joydipb/Documents/CMT307-Coursework-2-Group-19/LSTM_RNN_Final.ipynb#ch0000036?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(mindex)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joydipb/Documents/CMT307-Coursework-2-Group-19/LSTM_RNN_Final.ipynb#ch0000036?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(train_idx), \u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(valid_idx))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/joydipb/Documents/CMT307-Coursework-2-Group-19/LSTM_RNN_Final.ipynb#ch0000036?line=25'>26</a>\u001b[0m model, y_pred_valid\u001b[39m=\u001b[39m RNN_LSTM(train_data, valid_data) \u001b[39m# How to pass and train the categorical data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joydipb/Documents/CMT307-Coursework-2-Group-19/LSTM_RNN_Final.ipynb#ch0000036?line=28'>29</a>\u001b[0m y_valid_pred_total[valid_idx] \u001b[39m=\u001b[39m y_pred_valid\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joydipb/Documents/CMT307-Coursework-2-Group-19/LSTM_RNN_Final.ipynb#ch0000036?line=29'>30</a>\u001b[0m exec(\u001b[39m'\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(groupNum_train) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.append([mindex, model])\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;32m/home/joydipb/Documents/CMT307-Coursework-2-Group-19/LSTM_RNN_Final.ipynb Cell 35'\u001b[0m in \u001b[0;36mRNN_LSTM\u001b[0;34m(train, val)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joydipb/Documents/CMT307-Coursework-2-Group-19/LSTM_RNN_Final.ipynb#ch0000034?line=16'>17</a>\u001b[0m \u001b[39m# Fit model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joydipb/Documents/CMT307-Coursework-2-Group-19/LSTM_RNN_Final.ipynb#ch0000034?line=17'>18</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train, epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, batch_size \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m, validation_data \u001b[39m=\u001b[39m (X_valid, y_valid), callbacks \u001b[39m=\u001b[39m [early_stop, model_checkpoint])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/joydipb/Documents/CMT307-Coursework-2-Group-19/LSTM_RNN_Final.ipynb#ch0000034?line=19'>20</a>\u001b[0m y_pred_valid \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_valid)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joydipb/Documents/CMT307-Coursework-2-Group-19/LSTM_RNN_Final.ipynb#ch0000034?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m---------- Evaluation on Training Data ----------\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joydipb/Documents/CMT307-Coursework-2-Group-19/LSTM_RNN_Final.ipynb#ch0000034?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMSE: \u001b[39m\u001b[39m\"\u001b[39m, mean_squared_error(y_valid, y_pred_valid))\n",
            "File \u001b[0;32m/usr/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///usr/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///usr/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for groupNum_train in building_meta_df['groupNum_train'].unique():\n",
        "    X_train, y_train = create_X_y(train_df, groupNum_train=groupNum_train)\n",
        "    y_valid_pred_total = np.zeros(X_train.shape[0])\n",
        "    gc.collect()\n",
        "    print('groupNum_train', groupNum_train, X_train.shape)\n",
        "\n",
        "    cat_features = [X_train.columns.get_loc(\n",
        "        cat_col) for cat_col in category_cols]\n",
        "    print('cat_features', cat_features)\n",
        "\n",
        "    exec('models' + str(groupNum_train) + '=[]')\n",
        "\n",
        "    train_df_site = train_df[train_df['groupNum_train']\n",
        "                             == groupNum_train].copy()\n",
        "\n",
        "    \n",
        "    for train_idx, valid_idx in kf.split(train_df_site, train_df_site['building_id']):\n",
        "        train_data = X_train.iloc[train_idx, :], y_train[train_idx]\n",
        "        valid_data = X_train.iloc[valid_idx, :], y_train[valid_idx]\n",
        "\n",
        "        mindex = train_df_site.iloc[valid_idx, :].month.unique()\n",
        "        print(mindex)\n",
        "\n",
        "        print('train', len(train_idx), 'valid', len(valid_idx))\n",
        "    \n",
        "        model, y_pred_valid= RNN_LSTM(train_data, valid_data) # How to pass and train the categorical data\n",
        "        \n",
        "        \n",
        "        y_valid_pred_total[valid_idx] = y_pred_valid.reshape(-1)\n",
        "        exec('models' + str(groupNum_train) + '.append([mindex, model])')\n",
        "        gc.collect()\n",
        "        if debug:\n",
        "            break\n",
        "\n",
        "    try:\n",
        "        sns.distplot(y_train)\n",
        "        sns.distplot(y_valid_pred_total)\n",
        "        plt.show()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    del X_train, y_train\n",
        "    gc.collect()\n",
        "\n",
        "    print('-------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction on test dataÂ¶\n",
        "\n",
        "print('loading...')\n",
        "test_df = pd.read_feather(root/'test.feather')\n",
        "weather_test_df = pd.read_feather(root/'weather_test.feather')\n",
        "\n",
        "print('Before Preprocessing ....')\n",
        "print('Shape of test data: ', test_df.shape)\n",
        "print('Shape of Weather test data: ', weather_test_df.shape)\n",
        "\n",
        "weather_test_df = weather_test_df.drop_duplicates(['timestamp', 'site_id'])\n",
        "set_local(weather_test_df)\n",
        "add_holiyday(weather_test_df)\n",
        "\n",
        "print('preprocessing building...')\n",
        "test_df['date'] = test_df['timestamp'].dt.date\n",
        "preprocess(test_df)\n",
        "\n",
        "\n",
        "print('preprocessing weather...')\n",
        "weather_test_df = weather_test_df.groupby('site_id').apply(\n",
        "    lambda group: group.interpolate(method='ffill', limit_direction='forward'))\n",
        "weather_test_df.groupby('site_id').apply(lambda group: group.isna().sum())\n",
        "\n",
        "add_sg(weather_test_df)\n",
        "\n",
        "add_lag_feature(weather_test_df, window=3)\n",
        "add_lag_feature(weather_test_df, window=72)\n",
        "\n",
        "test_df['bid_cnt'] = test_df.building_id.map(bid_map)\n",
        "\n",
        "test_df = test_df.merge(building_meta_df[['building_id', 'meter', 'groupNum_train','square_feet']], on=[\n",
        "                        'building_id', 'meter'], how='left')\n",
        "\n",
        "test_df['square_feet_np_log1p'] = np.log1p(test_df['square_feet'])\n",
        "\n",
        "print('reduce mem usage...')\n",
        "test_df = reduce_mem_usage(test_df, use_float16=True)\n",
        "weather_test_df = reduce_mem_usage(weather_test_df, use_float16=True)\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "sample_submission = pd.read_feather(\n",
        "    os.path.join(root, 'sample_submission.feather'))\n",
        "reduce_mem_usage(sample_submission)\n",
        "\n",
        "print(sample_submission.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_X(test_df, groupNum_train):\n",
        "\n",
        "    target_test_df = test_df[test_df['groupNum_train']\n",
        "                             == groupNum_train].copy()\n",
        "    # target_test_df = target_test_df.merge(df_groupNum_median, on=['timestamp'], how='left')\n",
        "    target_test_df = target_test_df.merge(\n",
        "        building_meta_df, on=['building_id', 'meter', 'groupNum_train'], how='left')\n",
        "    target_test_df = target_test_df.merge(\n",
        "        weather_test_df, on=['site_id', 'timestamp'], how='left')\n",
        "    #target_test_df['group_median_'+str(groupNum_train)] = np.nan\n",
        "\n",
        "    X_test = target_test_df[feature_cols + category_cols]\n",
        "\n",
        "    return X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def pred_all(X_test, models, batch_size=10000):\n",
        "    iterations = (X_test.shape[1] + batch_size - 1) // batch_size\n",
        "    print('iterations', iterations)\n",
        "\n",
        "    y_test_pred_total = np.zeros(X_test.shape[0])\n",
        "    for i, (mindex, model) in enumerate(models):\n",
        "        print(f'predicting {i}-th model')\n",
        "        for k in tqdm(range(iterations)):\n",
        "            y_pred_test = model.predict(\n",
        "                X_test[k*batch_size:(k+1)*batch_size]) #num_iteration=model.best_iteration\n",
        "            y_test_pred_total[k*batch_size:(k+1)*batch_size] += y_pred_test.reshape(-1)\n",
        "\n",
        "    y_test_pred_total /= len(models)\n",
        "    return y_test_pred_total\n",
        "\n",
        "\n",
        "def pred(X_test, models, batch_size=10000):\n",
        "        print('all pred')\n",
        "        return pred_all(X_test, models, batch_size=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for groupNum_train in building_meta_df['groupNum_train'].unique():\n",
        "    print('groupNum_train: ', groupNum_train)\n",
        "    X_test = create_X(test_df, groupNum_train=groupNum_train)\n",
        "    gc.collect()\n",
        "\n",
        "    exec('y_test= pred(X_test, models' + str(groupNum_train) + ')')\n",
        "\n",
        "    sns.distplot(y_test)\n",
        "    plt.show()\n",
        "\n",
        "    print(X_test.shape, y_test.shape)\n",
        "    sample_submission.loc[test_df[\"groupNum_train\"] ==\n",
        "                          groupNum_train, \"meter_reading\"] = np.expm1(y_test)\n",
        "\n",
        "    del X_test, y_test\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Site-0 Correction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/c/ashrae-energy-prediction/discussion/119261#latest-684102\n",
        "sample_submission.loc[(test_df.building_id.isin(site_0_bids)) & (test_df.meter == 0), 'meter_reading'] = sample_submission[(\n",
        "    test_df.building_id.isin(site_0_bids)) & (test_df.meter == 0)]['meter_reading'] * 3.4118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.log1p(sample_submission['meter_reading']).hist(bins=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not debug:\n",
        "    sample_submission.to_csv(\n",
        "        'submission_LSTM_RNN_firstRun.csv', index=False, float_format='%.4f')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not debug:\n",
        "\n",
        "  ! mkdir -p ~/.kaggle/ && \\\n",
        "    echo '{\"username\":\"joydipbhowmick\",\"key\":\"5bd4e6a1fec9fc7f8a93def26785a6d2\"}' > ~/.kaggle/kaggle.json && \\\n",
        "    chmod 600 ~/.kaggle/kaggle.json # Create a new direcory use the kaggle token key in that and make it read only to current user.\n",
        "  ! kaggle competitions submit -c ashrae-energy-prediction -f submission_LSTM_RNN_firstRun.csv -m \"LSTM RNN Model No Blend First test run - still in debug mode\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ML_Final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
