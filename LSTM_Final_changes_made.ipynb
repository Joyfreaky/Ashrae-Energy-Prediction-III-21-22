{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Final_changes_made.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model"
      ],
      "metadata": {
        "id": "oMdf4cdp6foJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define settings\n",
        "black_day = 10\n",
        "outlier = False\n",
        "rescale = False\n",
        "\n",
        "debug = False\n",
        "num_rounds = 200\n",
        "\n",
        "clip0 = False  # minus meter confirmed in test(site0 leak data)\n",
        "\n",
        "folds = 3  # 3, 6, 12\n",
        "# 6: 1.1069822104487446\n",
        "# 3: 1.102507841834652\n",
        "# 12: 1.1074824417420517\n",
        "\n",
        "use_ucf = False\n",
        "ucf_clip = False\n"
      ],
      "metadata": {
        "id": "x_3WOy386eVJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from pandas.api.types import is_categorical_dtype\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import utils\n",
        "from sklearn.model_selection import train_test_split,KFold,GroupKFold\n",
        "import gc\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras.layers as layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout, Activation, SimpleRNN\n",
        "from keras.optimizers import *\n",
        "from keras import Input\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "09amUCAW6mIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "README\n",
        "With getting processed data, this programme reads in as feather, and then converts\n",
        "it to CSV. Google colab RAM cannot handle reading them both in as CSV.\n",
        "'''\n",
        "# Load data\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path('/home/joydipb/Documents/CMT307-Coursework-2-Group-19')\n",
        "train_df = pd.read_feather(root/'train_df_processed.feather')\n",
        "\n",
        "# Verify type\n",
        "print('Type of train_data: ', type(train_df))\n",
        "#print('Type of test_data: ', type(test_data))\n"
      ],
      "metadata": {
        "id": "374yvp1q7Hoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect train data\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "Q76hWIoi7SmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce memory usage function\n",
        "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
        "# Modified to support timestamp type, categorical type\n",
        "# Modified to add option to use float16 or not. feather format does not support float16.\n",
        "\n",
        "\n",
        "def reduce_mem_usage(df, use_float16=False):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "\n",
        "    for col in df.columns:\n",
        "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
        "            # skip datetime type or categorical type\n",
        "            continue\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(\n",
        "        100 * (start_mem - end_mem) / start_mem))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "bVdmeKW57Xdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection\n",
        "category_cols = ['building_id', 'site_id', 'primary_use',\n",
        "                 'IsHoliday', 'groupNum_train']  # , 'meter'\n",
        "feature_cols = ['square_feet', 'year_built'] + [\n",
        "    'hour', 'weekend',\n",
        "    #    'day', # 'month' ,\n",
        "    #    'dayofweek',\n",
        "    #    'building_median'\n",
        "] + [\n",
        "    'air_temperature', 'cloud_coverage',\n",
        "    'dew_temperature', 'precip_depth_1_hr',\n",
        "    'sea_level_pressure',\n",
        "    #'wind_direction', 'wind_speed',\n",
        "    'air_temperature_mean_lag72',\n",
        "    'air_temperature_max_lag72', 'air_temperature_min_lag72',\n",
        "    'air_temperature_std_lag72', 'cloud_coverage_mean_lag72',\n",
        "    'dew_temperature_mean_lag72', 'precip_depth_1_hr_mean_lag72',\n",
        "    'sea_level_pressure_mean_lag72',\n",
        "    # 'wind_direction_mean_lag72',\n",
        "    'wind_speed_mean_lag72',\n",
        "    'air_temperature_mean_lag3',\n",
        "    'air_temperature_max_lag3',\n",
        "    'air_temperature_min_lag3', 'cloud_coverage_mean_lag3',\n",
        "    'dew_temperature_mean_lag3',\n",
        "    'precip_depth_1_hr_mean_lag3',\n",
        "    'sea_level_pressure_mean_lag3',\n",
        "    #    'wind_direction_mean_lag3', 'wind_speed_mean_lag3',\n",
        "    #    'floor_area',\n",
        "    'year_cnt', 'bid_cnt',\n",
        "    'dew_smooth', 'air_smooth',\n",
        "    'dew_diff', 'air_diff',\n",
        "    'dew_diff2', 'air_diff2'\n",
        "]\n"
      ],
      "metadata": {
        "id": "kC-QRLJ07cBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "--------------------------------------------------------------------------------\n",
        "IMPLEMENTATION\n",
        "'''"
      ],
      "metadata": {
        "id": "JoOvcM2u7jew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get X_train and y_train\n",
        "def create_X_y(train_df, groupNum_train):\n",
        "\n",
        "    target_train_df = train_df[train_df['groupNum_train']\n",
        "                               == groupNum_train].copy()\n",
        "    # target_train_df = target_train_df.merge(df_groupNum_median, on=['timestamp'], how='left')\n",
        "    # target_train_df['group_median_'+str(groupNum_train)] = np.nan\n",
        "\n",
        "    X_train = target_train_df[feature_cols + category_cols]\n",
        "    y_train = target_train_df['meter_reading_log1p'].values\n",
        "\n",
        "    del target_train_df\n",
        "    return X_train, y_train"
      ],
      "metadata": {
        "id": "cDG6ByOI7nc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define RMSLE specifically for neural network\n",
        "from keras import backend as k\n",
        "def NN_RMSLE(y_act, y_pred):\n",
        "  return k.sqrt(k.mean(k.square(y_pred - y_act)))"
      ],
      "metadata": {
        "id": "5qq9u6_s7ssS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to train LSTM model\n",
        "from keras.callbacks import EarlyStopping\n",
        "def RNN_LSTM(train, val ):\n",
        "       X_train, y_train = train\n",
        "       X_valid, y_valid = val\n",
        "       model = Sequential()\n",
        "       early_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 3)\n",
        "\n",
        "       # Add layers, etc\n",
        "       model.add(layers.Dense(512, activation = 'relu', input_shape = (X_train.shape[1], )))\n",
        "       model.add(Dense(1, activation = 'linear'))\n",
        "       model.compile(optimizer = 'adam', loss = NN_RMSLE)\n",
        "\n",
        "       # Fit model\n",
        "       model.fit(X_train, y_train, epochs = 10, batch_size = 10000, validation_data = (X_valid, y_valid), callbacks = early_stop)\n",
        "\n",
        "       y_pred_valid = model.predict(X_valid)\n",
        "\n",
        "       print('---------- Evaluation on Training Data ----------')\n",
        "       print(\"MSE: \", mean_squared_error(y_valid, y_pred_valid))\n",
        "       print(\"\")\n",
        "       # log = {'train/mae': model.best_score['training']['l2'],\n",
        "       #        'valid/mae': model.best_score['valid_1']['l2']}\n",
        "\n",
        "       return model, y_pred_valid\n"
      ],
      "metadata": {
        "id": "8bGAnAOR7vsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Begin kfold\n",
        "\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "seed = 666\n",
        "shuffle = False\n",
        "#kf = KFold(n_splits=folds, shuffle=shuffle, random_state=seed)\n",
        "#kf = GroupKFold(n_splits=folds)\n",
        "kf = StratifiedKFold(n_splits=folds)\n",
        "\n",
        "for groupNum_train in train_df['groupNum_train'].unique():\n",
        "    X_train, y_train = create_X_y(train_df, groupNum_train=groupNum_train)\n",
        "    y_valid_pred_total = np.zeros(X_train.shape[0])\n",
        "    gc.collect()\n",
        "    print('groupNum_train', groupNum_train, X_train.shape)\n",
        "\n",
        "    cat_features = [X_train.columns.get_loc(\n",
        "        cat_col) for cat_col in category_cols]\n",
        "    print('cat_features', cat_features)\n",
        "\n",
        "    exec('models' + str(groupNum_train) + '=[]')\n",
        "\n",
        "    train_df_site = train_df[train_df['groupNum_train']\n",
        "                             == groupNum_train].copy()\n",
        "\n",
        "    # for train_idx, valid_idx in kf.split(X_train, y_train):\n",
        "    # for train_idx, valid_idx in kf.split(X_train, y_train, groups=get_groups(train_df, groupNum_train)):\n",
        "    for train_idx, valid_idx in kf.split(train_df_site, train_df_site['building_id']):\n",
        "        train_data = X_train.iloc[train_idx, :], y_train[train_idx]\n",
        "        valid_data = X_train.iloc[valid_idx, :], y_train[valid_idx]\n",
        "\n",
        "        mindex = train_df_site.iloc[valid_idx, :].month.unique()\n",
        "        print(mindex)\n",
        "\n",
        "        print('train', len(train_idx), 'valid', len(valid_idx))\n",
        "    #     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n",
        "        # model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols,\n",
        "        #                                     num_rounds=num_rounds, lr=0.05, bf=0.7)\n",
        "        model, y_pred_valid= RNN_LSTM(train_data, valid_data)\n",
        "        print(y_pred_valid.shape)\n",
        "        \n",
        "        y_valid_pred_total[valid_idx] = y_pred_valid.reshape(-1)\n",
        "        exec('models' + str(groupNum_train) + '.append([mindex, model])')\n",
        "        gc.collect()\n",
        "        if debug:\n",
        "            break\n",
        "\n",
        "    try:\n",
        "        sns.distplot(y_train)\n",
        "        sns.distplot(y_valid_pred_total)\n",
        "        plt.show()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    del X_train, y_train\n",
        "    gc.collect()\n",
        "\n",
        "    print('-------------------------------------------------------------')\n",
        "\n",
        "del train_df\n",
        "del train_df_site\n",
        "del train_idx\n",
        "del valid_idx\n",
        "del train_data\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "bXhWULNS74zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare sample submission\n",
        "sample_submission = pd.read_feather(\n",
        "    os.path.join(root, 'sample_submission.feather'))\n",
        "reduce_mem_usage(sample_submission)\n",
        "\n",
        "print(sample_submission.shape)"
      ],
      "metadata": {
        "id": "5PYotiiX7-jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to return X_test data\n",
        "def create_X(test_df, groupNum_train):\n",
        "\n",
        "    target_test_df = test_df[test_df['groupNum_train']\n",
        "                             == groupNum_train].copy()\n",
        "    # target_test_df = target_test_df.merge(df_groupNum_median, on=['timestamp'], how='left')\n",
        "    target_test_df = target_test_df.merge(\n",
        "        building_meta_df, on=['building_id', 'meter', 'groupNum_train'], how='left')\n",
        "    target_test_df = target_test_df.merge(\n",
        "        weather_test_df, on=['site_id', 'timestamp'], how='left')\n",
        "    #target_test_df['group_median_'+str(groupNum_train)] = np.nan\n",
        "\n",
        "    X_test = target_test_df[feature_cols + category_cols]\n",
        "\n",
        "    return X_test"
      ],
      "metadata": {
        "id": "EJ8e-xMh8TAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get test data\n",
        "test_df = pd.read_feather(\n",
        "    os.path.join(root, 'test_df_processed.feather'))\n",
        "#reduce_mem_usage(test_df)\n",
        "\n",
        "building_meta_df = pd.read_feather(\n",
        "    os.path.join(root, 'building_meta_df_processed.feather'))\n",
        "#reduce_mem_usage(building_meta_df)\n",
        "\n",
        "weather_test_df = pd.read_feather(\n",
        "    os.path.join(root, 'weather_test_df_processed.feather'))\n",
        "#reduce_mem_usage(weather_test_df)\n",
        "\n",
        "\n",
        "\n",
        "print(test_df.shape)\n",
        "print(building_meta_df.shape)\n",
        "print(weather_test_df.shape)"
      ],
      "metadata": {
        "id": "D7cPviH48Zii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "def pred_all(X_test, models, batch_size=10000):\n",
        "    iterations = (X_test.shape[1] + batch_size - 1) // batch_size\n",
        "    print('iterations', iterations)\n",
        "\n",
        "    y_test_pred_total = np.zeros(X_test.shape[0])\n",
        "    for i, (mindex, model) in enumerate(models):\n",
        "        print(f'predicting {i}-th model')\n",
        "        for k in tqdm(range(iterations)):\n",
        "            y_pred_test = model.predict(\n",
        "                X_test[k*batch_size:(k+1)*batch_size]) #num_iteration=model.best_iteration\n",
        "            y_test_pred_total[k*batch_size:(k+1)*batch_size] += y_pred_test.reshape(-1)\n",
        "\n",
        "    y_test_pred_total /= len(models)\n",
        "    return y_test_pred_total\n",
        "\n",
        "\n",
        "def pred(X_test, models, batch_size=10000):\n",
        "    if predmode == 'valid':\n",
        "        print('valid pred')\n",
        "        return pred_valid(X_test, models, batch_size=10000)\n",
        "    elif predmode == 'train':\n",
        "        print('train pred')\n",
        "        return pred_train(X_test, models, batch_size=10000)\n",
        "    else:\n",
        "        print('all pred')\n",
        "        return pred_all(X_test, models, batch_size=10000)\n",
        "\n",
        "for groupNum_train in building_meta_df['groupNum_train'].unique():\n",
        "    print('groupNum_train: ', groupNum_train)\n",
        "    X_test = create_X(test_df, groupNum_train=groupNum_train)\n",
        "    gc.collect()\n",
        "\n",
        "    exec('y_test= pred(X_test, models' + str(groupNum_train) + ')')\n",
        "\n",
        "    sns.distplot(y_test)\n",
        "    plt.show()\n",
        "\n",
        "    print(X_test.shape, y_test.shape)\n",
        "    sample_submission.loc[test_df[\"groupNum_train\"] ==\n",
        "                          groupNum_train, \"meter_reading\"] = np.expm1(y_test)\n",
        "\n",
        "    del X_test, y_test\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "id": "F8jaMqp48ihB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCchI8Lp6bbv"
      },
      "outputs": [],
      "source": [
        "# Submit\n",
        "\n",
        "#Site-0 Correction ## This line returns an error when left in script\n",
        "\n",
        "train_df = pd.read_feather(root/'train.feather')\n",
        "\n",
        "site_0_bids = building_meta_df[building_meta_df.site_id ==\n",
        "                               0].building_id.unique()\n",
        "print(len(site_0_bids), len(\n",
        "    train_df[train_df.building_id.isin(site_0_bids)].building_id.unique()))\n",
        "train_df[train_df.building_id.isin(\n",
        "    site_0_bids) & (train_df.meter == 0)].head(50)\n",
        "\n",
        "# https://www.kaggle.com/c/ashrae-energy-prediction/discussion/119261#latest-684102\n",
        "sample_submission.loc[(test_df.building_id.isin(site_0_bids)) & (test_df.meter == 0), 'meter_reading'] = sample_submission[(\n",
        "    test_df.building_id.isin(site_0_bids)) & (test_df.meter == 0)]['meter_reading'] * 3.4118\n",
        "\n",
        "sample_submission.head()\n",
        "\n",
        "sample_submission.tail()\n",
        "\n",
        "print('Shape of Sample Submission', sample_submission.shape)\n",
        "\n",
        "np.log1p(sample_submission['meter_reading']).hist(bins=100)\n",
        "\n",
        "if not debug:\n",
        "    sample_submission.to_csv(\n",
        "        'submission_LSTM_RNN_firstRun.csv', index=False, float_format='%.4f')\n",
        "\n",
        "! mkdir -p ~/.kaggle/ && \\\n",
        "  echo '{\"username\":\"joydipbhowmick\",\"key\":\"5bd4e6a1fec9fc7f8a93def26785a6d2\"}' > ~/.kaggle/kaggle.json && \\\n",
        "  chmod 600 ~/.kaggle/kaggle.json # Create a new direcory use the kaggle token key in that and make it read only to current user.\n",
        "! kaggle competitions submit -c ashrae-energy-prediction -f submission_LSTM_RNN_firstRun.csv -m \"LSTM RNN Model No Blend First test run - still in debug mode\"\n",
        "\n"
      ]
    }
  ]
}